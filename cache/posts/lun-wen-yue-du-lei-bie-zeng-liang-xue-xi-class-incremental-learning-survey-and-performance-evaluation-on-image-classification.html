<h2>简介</h2>
<p>In most incremental learning scenarios, tasks are presented to a learner in a sequence of delineated <em>training sessions</em> during which data from a single task is available for learning.
After each training session, the learner should be capable of performing all perviously seen tasks on unseen data.</p>
<blockquote>
<p>在增量学习场景下, 每一轮训练针对一个任务进行训练, 在训练结束后, 要求模型能对前面训练过的所有任务进行完成</p>
</blockquote>
<p>The main challenge in incremental learning is to learn from data from the current task in a way that prevents forgetting of previously learned tasks.</p>
<blockquote>
<p>增量学习的主要问题就是避免当前训练对以前任务的遗忘</p>
</blockquote>
<p>这一点和迁移学习类似但也有所不同, 迁移学习不要求对以前训练任务的知识实现保留</p>
<p>They aim to exploit knowledge from previous classes to improve learning of new ones(<em>forward transfer</em>), as well as exploiting new data to improve performance on previous tasks(<em>backward transfer</em>)</p>
<blockquote>
<p>用旧知识帮助学习新知识, 用新知识巩固旧知识, sounds make sense</p>
</blockquote>
<h2>一些常见概念</h2>
<h3>task-IL vs class-IL</h3>
<p>task-IL: task-incremental learning, 在inference的时候, 会给定样本的task ID</p>
<p>class-IL: class-incremental learning, 在inference的时候, 不给定样本的task ID</p>
<p>这里还是有些不清楚</p>
<h3>SIL vs CIL vs FIL</h3>
<ol>
<li>SIL</li>
</ol>
<p>问题：由于新数据的各种原因，样本的特征值可能会改变，每个类别的比例也会改变。这些都会影响分类的准确率。</p>
<p>任务：因此，需要确保在现有知识的情况下，通过新样本的增量学习来提取新知识，融合新旧知识以提高分类的准确性。</p>
<ol>
<li>
<p>CIL
任务：识别新类，并将其加入现有类别的集合中，提升分类的准确性和智能。</p>
</li>
<li>
<p>FIL
一些新的属性特征能够将分类提升到一个很大的程度，并提升分类准确率。</p>
</li>
</ol>
<p>任务：在现有特征空间的基础上，加入新的属性特征，构建新的特征空间，提升分类准确率。</p>
<h2>问题</h2>
<p>catastrophic forgetting(灾难性遗忘), 为了克服灾难性遗忘, 我们一方面希望模型能从新数据中有效学习新知识, 但另一方面又必须防止新输入的数据对已有知识的显著干扰(稳定性), 即<strong>稳定性-可塑性困境</strong>(stability-plasticity dilemma)</p>
<p>增量学习的主要研究目的就是在计算和存储资源有限的条件下, 在稳定性-可塑性困境中寻找最佳平衡点</p>
<h2>方法</h2>
<ol>
<li>
<p>正则(regularization)</p>
</li>
<li>
<p>回放(rehearsal)</p>
</li>
<li>
<p>参数隔离(parameter isolation)</p>
</li>
</ol>